# BlackboxNLP 2022

There will be a fifth edition of BlackboxNLP! 
It will be collocated with EMNLP 2022.

## Important dates

- July 15, 2022 (tentative) -- ARR submission deadline (via [ARR](https://openreview.net/group?id=aclweb.org/ACL/ARR/2022)).
- September 7, 2022 -- Direct submission deadline (via [OpenReview](https://openreview.net/group?id=EMNLP/2022/Workshop/BlackboxNLP))
- October 2, 2022 -- ARR commitment deadline (also via [OpenReview](https://openreview.net/group?id=EMNLP/2022/Workshop/BlackboxNLP)).
- October 9, 2022 -- Notification of acceptance.
- October 16, 2022 -- Camera-ready papers due.
- December 7 or 8, 2022 (exact date/time TBD) -- Workshop (hybrid).

All deadlines are 11:59pm UTC-12 ("anywhere on earth").

## Workshop description

Neural networks have rapidly become a central component in NLP systems in the last few years. 
The improvement in accuracy and performance brought by the introduction of neural networks has typically come at the cost of our understanding of the system: How do we assess what the representations and computations are that the network learns? 
The goal of this workshop is to bring together people who are attempting to peek inside the neural network black box, taking inspiration from machine learning, psychology, linguistics, and neuroscience.

The topics of the workshop will include, but are not limited to:

- Applying analysis techniques from neuroscience to analyse high-dimensional vector representations (such as Haxby et al., 2001; Kriegeskorte, 2008) in artificial neural networks;
- Analyzing the network's response to strategically chosen inputs in order to infer the linguistic generalizations that the network has acquired (e.g., Linzen et al., 2016; Hupkes et al., 2020; Dankers et al., 2021);
- Examining the performance of the network on simplified or formal languages (e.g., Hupkes et al., 2018; Lake et al., 2018);
- Proposing modifications to neural network architectures that can make them more interpretable (e.g., Palanki et al., 2018);
- Scaling up neural network analysis techniques developed in the connectionist literature in the 1990s (Elman, 1991); 
- Testing whether interpretable information can be decoded from intermediate representations (e.g., Adi et al.,  2017; Chrupala et al., 2017; Hupkes et al., 2017, Conneau et al., 2018);
- Translating insights on neural networks interpretation from the vision domain (e.g., Zeiler & Fergus, 2014) to language;
- Explaining model predictions (e.g., Lei et al., 2016; Alvarez-Melis & Jaakkola, 2017): What are ways to explain specific decisions made by neural networks?
- Adversarial examples in NLP (e.g., Ebrahimi et al., 2018; Belinkov & Bisk, 2018): How to generate them and how to evaluate their quality?
- Open-source tools for analyzing neural networks in NLP (e.g., Strobelt et al., 2018; Rikters, 2018).
- Evaluation of analysis results: How do we know that the analysis is valid?
- Analysing the linguistic properties captured by contextualised word representations (e.g. Aina et al 2019, Bommasani et al., 2020)
- Analysing learning and inference mechanisms of neural networks, such as memory and attention (e.g. Abnar and Zuidema, 2020, Serrano and Smith, 2019, Haviv et al., 2019)

## Call for Papers
We will be accepting 8-page archival submissions and non-archival extended abstracts. More details TBA.

## Previous workshops

- [BlackboxNLP 2018](https://blackboxnlp.github.io/2018/) (at EMNLP 2018)
- [BlackboxNLP 2019](https://blackboxnlp.github.io/2019/) (at ACL 2019)
- [BlackboxNLP 2020](https://blackboxnlp.github.io/2020/) (at EMNLP 2020)
- [BlackboxNLP 2021](https://blackboxnlp.github.io/2021/) (at EMNLP 2021)

## Organizers

### Jasmijn Bastings
Jasmijn Bastings (bastings[-at-]google.com) is a researcher at Google in Amsterdam. 
She got her PhD from the University of Amsterdam on Interpretable and Linguistically-informed Deep Learning for NLP. 
Jasmijn's current research focuses on interpretable NLP models and predictions, and she authored two BlackboxNLP papers (2018, 2020) on generalisation and saliency methods, as well as an ACL paper (2019) on interpretable neural predictions using differentiable binary variables.

### Yonatan Belinkov
Yonatan Belinkov (belinkov@technion.ac.il) is an assistant professor at the Technion. 
He has previosuly been a Postdoctoral Fellow at Harvard and MIT. 
His recent research focuses on interpretability and robustness of neural network models of language. 
His research has been published at leading NLP and ML venues. 
His PhD dissertation at MIT analyzed internal language representations in deep learning models.
He has been awarded the Harvard Mind, Brain, and Behavior Postdoctoral Fellowship and the Azrieli Early Career Faculty Fellowship.
He co-organised BlackboxNLP in 2019, 2020, and 2021, as well as the 1st and 2nd machine translation robustness tasks at WMT.

### Yanai Elazar
Yanai Elazar (yanaiela@gmail.com) is a PhD student at Bar-Ilan University. 
His research focus is interpretability and analysis methods for NLP. 
His research showed how demographics and linguistics phenomena are encoded in modelsâ€™ representations, and how more abstract capabilities, such as commonsense and reasoning, are manifested, and being used by models. 
In addition to regularly serving on program committees, he co-organized the HAMLETS workshop at NeurIPS 2020.

### Dieuwke Hupkes
Dieuwke Hupkes (dieuwkehupkes@fb.com) is a research scientist at Facebook AI Research.
The main focus of her research is understanding how neural networks  generalise, considering specifically on how they can understand and learn grammar, structure and compositionality. 
Developing methods to interpret and interact with neural networks has therefore been an important area of focus in her research.
She authored many articles directly relevant to the workshop and has co-organised the previous three editions of BlackboxNLP. 

### Naomi Saphra
Naomi Saphra} (nsaphra@nyu.edu) is a postdoc at New York University. 
Their research is on understanding the training dynamics of language models, from the standpoint of linguistic structure acquisition. 
Their relevant work has been published at NAACL and EMNLP, and they have served on the organizing committee for the Workshop on Representation Learning for NLP (RepL4NLP). 

### Sarah Wiegreffe
Sarah Wiegreffe (saw@gatech.edu) is a PhD candidate at Georgia Institute of Technology. 
Her research focuses on improved modeling and analysis of explanations from neural models along the axes of faithfulness and human acceptability, with a recent focus on free-text explanations. 
Her research on interpretability has been published at leading ML and NLP conferences. 
She served as a publicity chair for NAACL 2021 and frequently serves on conference program committees.

## Anti-Harassment Policy
BlackboxNLP 2022 adheres to the [ACL Anti-Harassment Policy](https://www.aclweb.org/adminwiki/sphp?title=Anti-Harassment_Policy).
