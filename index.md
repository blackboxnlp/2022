# BlackboxNLP 2022

There will be a fifth edition of BlackboxNLP! 
It will be collocated with EMNLP 2022.

## Important dates

- July 15, 2022 -- ARR submission deadline (via [ARR](https://openreview.net/group?id=aclweb.org/ACL/ARR/2022)).
- September 7, 2022 -- Direct submission deadline (via [OpenReview](https://openreview.net/group?id=EMNLP/2022/Workshop/BlackboxNLP))
- October 2, 2022 -- ARR commitment deadline (via [OpenReview](https://openreview.net/group?id=EMNLP/2022/Workshop/BlackboxNLP)).
- **October 11, 2022** -- Notification of acceptance. (This was delayed by two days to make sure emergency reviews are taken into account.)
- October 16, 2022 -- Camera-ready papers due.
- December 8, 2022 -- Workshop (hybrid).

All deadlines are 11:59pm UTC-12 ("anywhere on earth").

## Workshop description

Many recent performance improvements in NLP have come at the cost of understanding of the systems. How do we assess what representations and computations models learn? How do we formalize desirable properties of interpretable models, and measure the extent to which existing models achieve them? How can we build models that better encode these properties? What can new or existing tools tell us about systems’ inductive biases?

The goal of this workshop is to bring together researchers focused on interpreting and explaining NLP models by taking inspiration from machine learning, psychology, linguistics, and neuroscience. We hope the workshop will serve as an interdisciplinary meetup that allows for cross-collaboration.

The topics of the workshop include, but are not limited to:
- Explanation methods such as saliency, attribution, free-text explanations, or explanations with structured properties
- Probing methods for testing whether models have acquired or represent certain linguistic properties
- Applying analysis techniques from other disciplines (e.g., neuroscience or computer vision)
- Examining model performance on simplified or formal languages
- More interpretable model architectures
- Open-source tools for analysis, visualization, or explanation;
- Evaluation of explanation methods
- Opinion pieces about the state of explainable NLP

Feel free to reach out to the organizers at the email below if you are not sure whether a specific topic is well-suited for submission.

## Call for Papers
All submissions should use the ACL [templates](https://github.com/acl-org/acl-style-files) and formatting requirements specified by [ACL Rolling Review](https://aclrollingreview.org/), and should be fully anonymized. Submissions of both types can be made through [OpenReview](https://openreview.net/group?id=EMNLP/2022/Workshop/BlackboxNLP).

### Submission Types
- **Archival papers** of up to 8 pages + references. These are papers reporting on completed, original and unpublished research, and can be submitted **either with or without ARR reviews** by selecting the appropriate box on the submission form. An optional appendix may appear after the references in the same pdf file. If you do not include ARR reviews with your submission, it will be reviewed by reviewers specific to the BlackBoxNLP workshop. Accepted papers are expected to be presented at the workshop and will be published in the workshop proceedings of the ACL Anthology, meaning they cannot be published elsewhere. They should report on obtained results rather than intended work. Broader Impacts/Ethics and Limitations sections are optional and can be included on a 9th page.
- **Non-archival extended abstracts** of 2 pages + references. These may report on work in progress or may be cross submissions of work that has already appeared (or is scheduled to appear) in another venue in 2021-2022. Abstract titles will be posted on the workshop website but will not be included in the proceedings.

Accepted submissions will be presented at the workshop: most as posters, some as oral presentations (determined by the program committee).

### Dual Submissions and Preprints
Dual submissions are **not** allowed for the archival track. Papers posted to preprint servers such as arxiv can be submitted without any restrictions on when they were posted.

### Camera-ready information
Authors of accepted archival papers should upload the final version of their paper to the submission system by the camera-ready deadline. Authors may use one extra page to address reviewer comments, for a total of nine pages + references. Broader Impacts/Ethics and Limitations sections are optional and can be included on a 10th page.

## Contact
Please contact the organizers at blackboxnlp@googlegroups.com for any questions.

## Previous workshops

- [BlackboxNLP 2018](https://blackboxnlp.github.io/2018/) (at EMNLP 2018)
- [BlackboxNLP 2019](https://blackboxnlp.github.io/2019/) (at ACL 2019)
- [BlackboxNLP 2020](https://blackboxnlp.github.io/2020/) (at EMNLP 2020)
- [BlackboxNLP 2021](https://blackboxnlp.github.io/2021/) (at EMNLP 2021)

## Sponsors

Blackbox NLP 2022 is sponsored by:

<img src="https://raw.githubusercontent.com/blackboxnlp/blackboxnlp.github.io/main/Google%20Logo.png" height="100px" alt="Google logo" />

## Organizers

### Jasmijn Bastings
Jasmijn Bastings (last name at google.com) is a researcher at Google in Amsterdam. 
She got her PhD from the University of Amsterdam on Interpretable and Linguistically-informed Deep Learning for NLP. 
Jasmijn's current research focuses on interpretable NLP models and predictions, and she authored two BlackboxNLP papers (2018, 2020) on generalisation and saliency methods, as well as an ACL paper (2019) on interpretable neural predictions using differentiable binary variables.

### Yonatan Belinkov
Yonatan Belinkov (belinkov@technion.ac.il) is an assistant professor at the Technion. 
He has previously been a Postdoctoral Fellow at Harvard and MIT.
His recent research focuses on interpretability and robustness of neural network models of language. 
His research has been published at leading NLP and ML venues. 
His PhD dissertation at MIT analyzed internal language representations in deep learning models.
He has been awarded the Harvard Mind, Brain, and Behavior Postdoctoral Fellowship and the Azrieli Early Career Faculty Fellowship.
He co-organised BlackboxNLP in 2019, 2020, and 2021, as well as the 1st and 2nd machine translation robustness tasks at WMT.

### Yanai Elazar
Yanai Elazar (yanaiela@gmail.com) is a postdoctoral researcher at AI2 & UW. 
His research focus is interpretability and analysis methods for NLP.
His research showed how demographics and linguistics phenomena are encoded in models’ representations, and how more abstract capabilities, such as commonsense and reasoning, are manifested, and being used by models.

### Dieuwke Hupkes
Dieuwke Hupkes (dieuwkehupkes@fb.com) is a research scientist at Facebook AI Research.
The main focus of her research is understanding how neural networks  generalise, considering specifically on how they can understand and learn grammar, structure and compositionality. 
Developing methods to interpret and interact with neural networks has therefore been an important area of focus in her research.
She authored many articles directly relevant to the workshop and has co-organised the previous three editions of BlackboxNLP. 

### Naomi Saphra
Naomi Saphra} (nsaphra@nyu.edu) is a postdoc at New York University. 
Their research is on understanding the training dynamics of language models, from the standpoint of linguistic structure acquisition. 
Their relevant work has been published at NAACL and EMNLP, and they have served on the organizing committee for the Workshop on Representation Learning for NLP (RepL4NLP). 

### Sarah Wiegreffe
Sarah Wiegreffe (sarahw at allenai.org) is a postdoctoral researcher at the Allen Institute for AI (AI2).
Her research focuses on improved modeling and analysis of explanations from neural models along the axes of faithfulness and human acceptability, with a recent focus on free-text explanations. 
Her research on interpretability has been published at leading ML and NLP conferences. 
She served as a publicity chair for NAACL 2021 and frequently serves on conference program committees.

## Anti-Harassment Policy
BlackboxNLP 2022 adheres to the [ACL Anti-Harassment Policy](https://www.aclweb.org/adminwiki/sphp?title=Anti-Harassment_Policy).
